# -*- coding: utf-8 -*-
"""Housing Price - Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/125eBnFPPsEQMbXR-4Q7Alc6Xb1jFGdqy

**House Prices - Advanced Regression Techniques**
 
                          **  Introduction Objective:**
The objective of the project is to perform advance regression techniques to predict the house price in Boston.

**Data Description:**

train.csv - the training set

test.csv - the test set

data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here

sample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms



**Table of Content:**

Fetch Dataset

Install & Import Libraries

Load Datasets

Exploratory Data Analysis

Feature Engineering

Model Development

Find Prediction

# 1. Fetch datasets from kaggle

# 2. Import Libraries
"""

# !pip install missingno

# !pip install optuna

# !pip install lazypredict==0.2.7

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
## Display all the columns of the dataframe
pd.pandas.set_option('display.max_columns',None)

from scipy import stats
from scipy.stats import norm, skew # for some statistics
import warnings # to ignore warning
from sklearn.preprocessing import RobustScaler, PowerTransformer, LabelEncoder
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split

import lazypredict
from lazypredict.Supervised import LazyRegressor
import optuna
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import Ridge, Lasso, ElasticNet, LassoCV, RidgeCV

from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
import xgboost as xgb
import lightgbm as lgb
import joblib

print("Library Imported!!")

"""# 3. Load Datasets"""

# load train and test dataset
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")

# combined train and test datasets
combined_df = pd.concat([train_df,test_df],axis=0)

"""# 4. EDA"""

train_df.head()

train_df.info()

"""**Checking NULL values**"""

train_df.isnull().sum().sort_values(ascending = False).head(30)

"""**Missing Value Heat Map**"""

msno.heatmap(train_df)

msno.bar(train_df)

msno.bar(test_df)

"""No of Missing values in Test set and Train set is Similar

**Dropping Columns with Many Empty values**
"""

combined_df = combined_df.drop('MiscFeature',axis=1)
combined_df = combined_df.drop('Fence',axis=1)
combined_df = combined_df.drop('PoolQC',axis=1)
combined_df = combined_df.drop('Alley',axis=1)

"""**Handling Column FireplaceQu**

Columns with Empty Fire Place can be considered as NA - No Fireplace
"""

combined_df.loc[combined_df['FireplaceQu'].isnull(),'FireplaceQu'] = 'NA'

train_df_temp = train_df.drop('SalePrice',axis=1)

"""Comparing the datatypes of train and test set"""

train_df_temp.dtypes.compare(test_df.dtypes)

"""No Difference in Datatypes of training and test


"""

train_df_temp.isnull().sum().compare(test_df.isnull().sum())

"""No Major Deviation between NULL values in train and test"""

list_numericalCol =[col for col in combined_df.columns if combined_df[col].dtypes != 'O' and col not in ['Id','YearBuilt','YearRemodAdd','GarageYrBlt','YrSold']]
combined_df[list_numericalCol]

List_categorical_Columns = [col for col in combined_df.columns if combined_df[col].dtypes == 'O']
train_df[List_categorical_Columns]

numerical_features = [col for col in combined_df.columns if combined_df[col].dtypes != 'O' and col not in ['Id']]
discrete_features = [col for col in numerical_features if len(combined_df[col].unique()) < 25 and col not in ['Id']]
continuous_features = [feature for feature in numerical_features if feature not in discrete_features+['Id']]
categorical_features = [col for col in combined_df.columns if combined_df[col].dtype == 'O']

print("Total Number of Numerical Columns : ",len(numerical_features))
print("Number of discrete features : ",len(discrete_features))
print("No of continuous features are : ", len(continuous_features))
print("Number of discrete features : ",len(categorical_features))



combined_df["Label"] = "test"
combined_df["Label"][:1460] = "train"

"""Hist Plot on Numerical Feature"""

f, axes = plt.subplots( 7,6 , figsize=(30, 30), sharex=False)
# print(type(f),type(axes))
for i, feature in enumerate(numerical_features):
    sns.histplot(data=combined_df, x = feature, hue="Label",ax=axes[i%7,i//7])

"""Dropping features with more than 98% of them has values 0"""

drop_columns = ['Id','LowQualFinSF','MiscVal','PoolArea','3SsnPorch']

for col in drop_columns:
  combined_df = combined_df.drop(col,axis=1)

"""Count Plot on Categorical Features"""

f, axes = plt.subplots( 11,4 , figsize=(30, 30), sharex=False)
# print(type(f),type(axes))
for i, feature in enumerate(categorical_features):
    sns.countplot(data=combined_df, x = feature, hue="Label",ax=axes[i%11,i//11])

"""Scatter Plot on Numerical Features against Target Feature"""

temp_num_col = [ col for col in numerical_features if col not in drop_columns]
f, axes = plt.subplots(7,6 , figsize=(30, 30), sharex=False)
for i, feature in enumerate(temp_num_col):
    sns.scatterplot(data=combined_df, x = feature, y= "SalePrice",ax=axes[i%7, i//7])

"""Box Plot on Categorical Features"""

f, axes = plt.subplots(7,7 , figsize=(30, 30), sharex=False)
for i, feature in enumerate(categorical_features):
    #feature = feature+'_code'
    sort_list = sorted(combined_df.groupby(feature)['SalePrice'].median().items(), key= lambda x:x[1], reverse = True)
    order_list = [x[0] for x in sort_list ]
    sns.boxplot(data = combined_df, x = feature, y = 'SalePrice', order=order_list, ax=axes[i%7, i//7])

"""Handling NULL values of Year Columns"""

col_yearColumns = ['GarageYrBlt','YrSold','YearBuilt','YearRemodAdd']

for col in col_yearColumns:
  combined_df.loc[combined_df[col].isnull(),col]= combined_df[col].mode()[0]

combined_df['GarageYrBlt'].sort_values(ascending = False).head(30)

"""Treating Outlier for GarageYrBlt"""

combined_df.loc[combined_df['GarageYrBlt']>2010,'GarageYrBlt'] = 2010

"""# Feature Engineering

Converting Year Columns to No of years Since
"""

for col in [col for col in col_yearColumns if col not in ['YrSold']]:
  combined_df['NoOfYear_'+col] = np.nan
  combined_df['NoOfYear_'+col] = combined_df['YrSold'] - combined_df[col]
  combined_df = combined_df.drop(col,axis=1)

"""Treating NULL values for Categorical Features"""

for col in categorical_features:
  combined_df.loc[combined_df[col].isnull(),col]= combined_df[col].mode()[0]

"""From the Box plot, The Utilities Column is predominantly having only one feature and hence Dropping the column"""

combined_df = combined_df.drop('Utilities',axis=1)

"""Combining Condition1 and Condition 2"""

combined_df['Conditions'] = np.where(combined_df['Condition1'] != combined_df['Condition2'], combined_df['Condition1'] +','+ combined_df['Condition2'], combined_df['Condition1'])

combined_df = combined_df.drop('Condition1',axis=1)
combined_df = combined_df.drop('Condition2',axis=1)

"""Spelling Correction"""

combined_df.loc[combined_df['Exterior1st'] == 'CmentBd','Exterior1st']='CemntBd'
combined_df.loc[combined_df['Exterior2nd'] == 'CmentBd','Exterior2nd']='CemntBd'
combined_df.loc[combined_df['Exterior1st'] == 'Wd Shng','Exterior1st']='WdShing'
combined_df.loc[combined_df['Exterior2nd'] == 'Wd Shng','Exterior2nd']='WdShing'

"""Combining Exterior1st  and Exterior2nd"""

combined_df['Exteriors'] = np.where(combined_df['Exterior1st'] != combined_df['Exterior2nd'], combined_df['Exterior1st'] +','+ combined_df['Exterior2nd'], combined_df['Exterior1st'])

combined_df = combined_df.drop('Exterior1st',axis=1)
combined_df = combined_df.drop('Exterior2nd',axis=1)

col_RankingEncoding = ['MSZoning','Street','LotShape','LandContour','LotConfig','LandSlope',
                       'BldgType','HouseStyle','RoofStyle','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond'
,'BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual'
,'Functional','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','SaleType','SaleCondition','RoofMatl','FireplaceQu']

col_OneHotEncoding = ['Conditions','Exteriors','Neighborhood']

"""From the Box plot, we notice that the Order of the categorical features ranks are almost in the order of Median values. Hence applying label encoding based on the order of the median values"""

# for col in Columns_labelEncoding:
def LableEncodingBasedOnMedian(columnName):
  global combined_df
  df = combined_df.groupby(columnName).agg({'SalePrice': ['median']}).apply(lambda x: x.sort_values(ascending=True))
  #print(df.head())
  combined_df[columnName+'_code']=np.nan
  for i,row in zip(range(0,len(df.index)),df.index):
    combined_df.loc[combined_df[columnName]==row,columnName+'_code']=i+1
  combined_df = combined_df.drop(columnName,axis=1)

"""Applying Label and One Hot Encoding"""

for col in col_RankingEncoding:
  LableEncodingBasedOnMedian(col)

for col in col_OneHotEncoding:
  combined_df = pd.concat([combined_df.drop(col, axis=1), combined_df[col].str.get_dummies(sep=",")], axis=1)

combined_df.isnull().sum().sort_values(ascending=False).head(30)

null_features_numerical = [col for col in combined_df.columns if combined_df[col].isnull().sum() > 0 and col not in categorical_features]
null_features_numerical

fig, axes = plt.subplots(4,3, figsize=(30, 30))
for i, feature in enumerate(null_features_numerical):
    ax = axes[i%4,i//4]
    sns.distplot(combined_df[feature], bins=20,kde_kws={'linewidth':3,'color':'red'},label="original", ax=ax)
    ax.axvline(x=combined_df[feature].mean(), linewidth=3, color='g', label="mean", alpha=0.5,)
    ax.axvline(x=combined_df[feature].median(), linewidth=3, color='y', label="median", alpha=0.5)
plt.show()

"""Based on the distribution above, we fill the NULL values for numerical columns with median Values"""

col_fill_median = ['LotFrontage','MasVnrArea','BsmtHalfBath','BsmtFullBath','TotalBsmtSF','BsmtUnfSF','GarageCars','GarageArea','BsmtFinSF2','BsmtFinSF1']

for col in col_fill_median:
  combined_df.loc[combined_df[col].isnull(),col] = combined_df[col].median()

"""Checking Skewness"""

temp_num_col = [col for col in numerical_features if col not in col_yearColumns and col not in drop_columns]

fig, axes = plt.subplots(11,3, figsize=(30, 60))
for i, feature in enumerate(temp_num_col):
    ax = axes[i%11,i//11]
    sns.distplot(combined_df[feature], bins=30,kde_kws={'linewidth':3,'color':'red'},label="original", ax=ax)
    ax.axvline(x=combined_df[feature].mean(), linewidth=3, color='g', label="mean", alpha=0.5,)
    ax.axvline(x=combined_df[feature].median(), linewidth=3, color='y', label="median", alpha=0.5)
    ax.set(xlabel=feature+' (Skewness): '+str(skew(combined_df[feature])))
plt.show()

continuous_col = [elem for elem in temp_num_col if elem not in ['MSSubClass','OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath'
,'FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','YrSold','Fireplaces','MoSold','GarageCars','TotRmsAbvGrd']]

skewed_feats = combined_df[continuous_col].apply(lambda x : skew(x)).sort_values(ascending = False)

skewed_feats

col_skewness = ['LotArea','BsmtFinSF2','EnclosedPorch','ScreenPorch','MasVnrArea','OpenPorchSF'
,'WoodDeckSF','LotFrontage','1stFlrSF','BsmtFinSF1','GrLivArea','TotalBsmtSF','BsmtUnfSF','2ndFlrSF']

for col in col_skewness:
    power = PowerTransformer(method='yeo-johnson', standardize=True)
    combined_df[[col]] = power.fit_transform(combined_df[[col]])

skewed_feats = combined_df[continuous_col].apply(lambda x : skew(x)).sort_values(ascending = False)
skewed_feats

train_df = combined_df.loc[combined_df['Label']=='train']
train_df = train_df.drop('Label',axis=1)

test_df = combined_df.loc[combined_df['Label']=='test']
test_df = test_df.drop('Label',axis=1)

X_train = train_df.drop('SalePrice', axis=1)
y_train = np.log1p(train_df['SalePrice'].values.ravel())
# y_train = train_df['SalePrice'].values.ravel()
X_test = test_df.drop('SalePrice', axis=1)

pre_precessing_pipeline = make_pipeline(RobustScaler(), 
                                        # VarianceThreshold(0.001),
                                       )

X_train = pre_precessing_pipeline.fit_transform(X_train)
X_test = pre_precessing_pipeline.transform(X_test)

def MAPE(target,predict):
  target = np.expm1(target)
  predict = np.expm1(predict)
  abs_error= np.absolute(target - predict)
  percent_abs_error = abs_error/target
  mean_abs_per_error = np.mean(percent_abs_error)
  return mean_abs_per_error

x_train1,x_test1,y_train1,y_test1=train_test_split(X_train,y_train,test_size=0.25)

reg= LazyRegressor(verbose=0,ignore_warnings=True,custom_metric=MAPE)
train,test=reg.fit(x_train1,x_test1,y_train1,y_test1)
test

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(x_train1,y_train1)
print(lr.score(x_train1,y_train1))

from xgboost import XGBRegressor

my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)
my_model.fit(x_train1, y_train1)
print(my_model.score(x_train1,y_train1))

print(MAPE(y_train1,my_model.predict(x_train1)))
print(MAPE(y_test1,my_model.predict(x_test1)))

print(np.expm1(y_train1[0:1]),np.expm1(lr.predict(x_train1[0:1])))
